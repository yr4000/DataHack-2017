{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yair Hadas\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "#uploading different packages- to remove the one we do not need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from sklearn.model_selection import LeaveOneOut,KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "'''\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "'''\n",
    "from sklearn import cross_validation, metrics \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.model_selection import LeaveOneOut,ShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from random import seed\n",
    "seed(123)\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "from create_features import *\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set path to imprt and save files from and in\n",
    "with open('PATHS.json') as json_data:\n",
    "    paths = json.load(json_data)\n",
    "\n",
    "#upload data\n",
    "train = pd.read_csv(paths[\"TRAIN_PATH\"],index_col='Unnamed: 0')\n",
    "test = pd.read_csv(paths[\"TEST_PATH\"],index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove labels names from data\n",
    "train=train.drop('targetName', 1)\n",
    "#remove unnecessary time cells from data\n",
    "col_names = list(train)\n",
    "for name in col_names:\n",
    "    if name[:4] == \"Time\":\n",
    "        train=train.drop(name, 1)\n",
    "        \n",
    "col_names = list(test)\n",
    "for name in col_names:\n",
    "    if name[:4] == \"Time\":\n",
    "        test=test.drop(name, 1)\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build a function vec_size which measures vector magnitude\n",
    "def vec_size(x,y,z):\n",
    "    return (np.sqrt(z**2+x**2+y**2))\n",
    "#create a df vel_mag with the magnitude of the velocity and val_mean which average the velocity of the samples(row)   \n",
    "def vel(data):    \n",
    "    vel_magn=pd.DataFrame(np.zeros(shape=(len(data),30)))\n",
    "    for i in range(30):\n",
    "        x = data[\"velX_\"+str(i)]\n",
    "        y = data[\"velY_\"+str(i)]\n",
    "        z = data[\"velZ_\"+str(i)]\n",
    "    #velocity magnitude matrix    \n",
    "        vel_magn[\"vel_magnitude_\"+str(i)] =vec_size(x,y,z)\n",
    "    return (vel_magn)\n",
    "\n",
    "vel_mag_train = vel(train)\n",
    "vel_mag_test = vel(test)\n",
    "#mean velocity magnitudevector\n",
    "vel_mean_train=np.mean(vel_mag_train, axis=1)\n",
    "vel_mean_test=np.mean(vel_mag_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a function to calculate the acceleration between each step\n",
    "def acc(data, vel_res):   \n",
    "    acc_df=pd.DataFrame(np.zeros(shape=(len(data),30)))\n",
    "    for i in range(0,29):\n",
    "        vel1=vel_res[\"vel_magnitude_\"+str(i)]\n",
    "        vel2=vel_res[\"vel_magnitude_\"+str(i+1)]\n",
    "        acc_df[\"accel_\"+str(i)]=vel2-vel1\n",
    "    return (acc_df) \n",
    "\n",
    "acc_df_train =acc(train, vel_mag_train)\n",
    "acc_df_test =acc(test, vel_mag_test)\n",
    "#mean acc \n",
    "acc_mean_train=np.mean(acc_df_train, axis=1)\n",
    "acc_mean_test=np.mean(acc_df_test, axis=1)\n",
    "#print (acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#angle calculation\n",
    "def calc_angle(data,pos_index): \n",
    "        x_prev = data[\"posX_\"+str(pos_index)] \n",
    "        x_curr = data[\"posX_\"+str(pos_index+1)]\n",
    "        \n",
    "        y_prev = data[\"posY_\"+str(pos_index)] \n",
    "        y_curr = data[\"posY_\"+str(pos_index+1)]\n",
    "        \n",
    "        z_prev = data[\"posZ_\"+str(pos_index)] \n",
    "        z_curr = data[\"posZ_\"+str(pos_index+1)]\n",
    "        \n",
    "        curr_point_vec = [x_curr-x_prev,y_curr-y_prev,z_curr-z_prev]\n",
    "        curr_point_vec_mag = vec_size(curr_point_vec[0],curr_point_vec[1],curr_point_vec[2])\n",
    "        curr_point_vec_norm = [curr_point_vec[0]/curr_point_vec_mag,curr_point_vec[1]/curr_point_vec_mag,curr_point_vec[2]/curr_point_vec_mag]\n",
    "        \n",
    "        plain_vec =[x_curr-x_prev,y_curr-y_prev,0] \n",
    "        plain_vec_mag = vec_size(plain_vec[0],plain_vec[1],0)\n",
    "        \n",
    "        plain_vec_norm = [plain_vec[0]/plain_vec_mag,plain_vec[1]/plain_vec_mag,0]\n",
    "        res = curr_point_vec_norm[0]*plain_vec_norm[0] +curr_point_vec_norm[1]*plain_vec_norm[1] +curr_point_vec_norm[2]* plain_vec_norm[2] \n",
    "        angle = np.arccos(res)\n",
    "        return (angle*180.0/ np.pi)\n",
    "    \n",
    "def calc_angle2(data,pos_index): #TODO: Need to pass the data set as argument. \n",
    "                    # Currently works only for train.\n",
    "        x_prev = data[\"posX_\"+str(pos_index)] \n",
    "        x_curr = data[\"posX_\"+str(pos_index+1)]\n",
    "        \n",
    "        y_prev = data[\"posY_\"+str(pos_index)] \n",
    "        y_curr = data[\"posY_\"+str(pos_index+1)]\n",
    "        \n",
    "        z_prev = data[\"posZ_\"+str(pos_index)] \n",
    "        z_curr = data[\"posZ_\"+str(pos_index+1)]\n",
    "        \n",
    "        \n",
    "        curr_point_vec = [x_curr-x_prev,y_curr-y_prev,z_curr-z_prev]\n",
    "        curr_point_vec_mag = vec_size(curr_point_vec[0],curr_point_vec[1],curr_point_vec[2])\n",
    "        \n",
    "        plain_vec =[x_curr-x_prev,y_curr-y_prev,0] \n",
    "        plain_vec_mag = vec_size(plain_vec[0],plain_vec[1],0)\n",
    "        \n",
    "        res = plain_vec_mag/curr_point_vec_mag\n",
    "        angle = np.arccos(res)\n",
    "        return (angle*180.0/ np.pi)\n",
    "    \n",
    "#run it on a whole df\n",
    "def angle(data, num_of_func = 1):    \n",
    "    angle_df=pd.DataFrame(np.zeros(shape=(len(data),29)))\n",
    "    for i in range(29): #TODO: Why range only until 29?\n",
    "        #print (train.iloc[:,i])\n",
    "        if num_of_func==1:\n",
    "            angle_df[\"angle_\"+str(i)] =calc_angle(data,i)\n",
    "        else:\n",
    "            angle_df[\"angle_\"+str(i)] =calc_angle2(data,i)\n",
    "    return angle_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "angle_df_train=angle(train,1)\n",
    "angle_df_test=angle(test,1)\n",
    "\n",
    "angle_df_train2 =angle(train,2)\n",
    "angle_df_test2 =angle(test,2)\n",
    "'''\n",
    "print(\"angle_df_train: \")\n",
    "print(angle_df_train.head())\n",
    "print(\"angle_df_train2: \")\n",
    "print(angle_df_train2.head())\n",
    "'''\n",
    "#calculate it mean\n",
    "angle_mean_train=np.mean(angle_df_train, axis=1)\n",
    "angle_mean_test=np.mean(angle_df_test, axis=1)\n",
    "#print (angle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a df to calculate the change in angles between each step\n",
    "def angle_che(data,angle_df):\n",
    "    angle_change_df=pd.DataFrame(np.zeros(shape=(len(data),30)))\n",
    "    for i in range(0,28):\n",
    "        ang1=angle_df[\"angle_\"+str(i)]\n",
    "        ang2=angle_df[\"angle_\"+str(i+1)]\n",
    "        angle_change_df[\"angle_change_\"+str(i)]=np.abs(ang2-ang1)\n",
    "        \n",
    "    return(angle_change_df)\n",
    "#print (angle_change_df)\n",
    "\n",
    "\n",
    "angle_change_df_train = angle_che(train,angle_df_train)\n",
    "angle_change_df_test = angle_che(test,angle_df_test)\n",
    "\n",
    "#calculate the mean\n",
    "angle_change_mean_train=np.mean(angle_change_df_train, axis=1)\n",
    "angle_change_mean_test=np.mean(angle_change_df_test, axis=1)\n",
    "#print (angle_change_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count how many time steps each samples has (as non NaN)\n",
    "def count_time(data):\n",
    "    res = pd.DataFrame()\n",
    "    time_vec=[]\n",
    "    for i in range(0,len(data)):\n",
    "        sample=data.iloc[i,:]\n",
    "        time_vec.append((29-sample.isnull().sum()/6)/2)\n",
    "    res[\"time_count\"] = time_vec\n",
    "    return res\n",
    "time_epoch_train = count_time(train)\n",
    "time_epoch_test = count_time(test) \n",
    "#time_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get goes_up and goes_down features:\n",
    "up_and_down = create_go_up_and_go_down(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get parbola parameters for each features\n",
    "parabola_params = calc_parabola_params(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create new variable to store the data\n",
    "new_train =train.copy(deep=True)\n",
    "new_test =test.copy(deep=True)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove Position columns from data\n",
    "col_names=list(new_train)\n",
    "for name in col_names:\n",
    "    if str(name)[:3] == \"pos\":\n",
    "        new_train=new_train.drop(name, 1)\n",
    "\n",
    "col_names=list(new_test)\n",
    "for name in col_names:\n",
    "    if  str(name)[:3] == \"pos\":\n",
    "        new_test=new_test.drop(name, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a vector of all the matrices and array which we which to add to the data\n",
    "\n",
    "df_add_vec_train=[new_train,vel_mag_train,\n",
    "                  vel_mean_train,acc_df_train,\n",
    "                  acc_mean_train,angle_df_train,\n",
    "                  angle_mean_train,angle_change_df_train,\n",
    "                  angle_change_mean_train,\n",
    "                  time_epoch_train, up_and_down, parabola_params]\n",
    "\n",
    "df_add_vec_test=[new_test,vel_mag_test,\n",
    "                  vel_mean_test,acc_df_test,\n",
    "                  acc_mean_test,angle_df_test,\n",
    "                  angle_mean_test,angle_change_df_test,\n",
    "                  angle_change_mean_test,\n",
    "                  time_epoch_test]\n",
    "\n",
    "#merge them to the data\n",
    "final_train = pd.concat(df_add_vec_train, axis=1)\n",
    "final_test = pd.concat(df_add_vec_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keep the clean data in CSV files\n",
    "final_train.to_csv(\"final_train.csv\",header=True,index=True)\n",
    "final_test.to_csv(\"final_test.csv\",header=True,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#rearenge the column name to enable modeling\n",
    "final_train.columns=list(final_train)[:91]+[i for i in range(1,125)]\n",
    "final_test.columns=list(final_train)[:90]+[i for i in range(1,125)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seperatre training data into features and labels\n",
    "Y =pd.DataFrame(final_train['class'])\n",
    "X =final_train.drop('class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data to training and testinf sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "y_train,y_test=np.ravel(y_train),np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# in case needed it is posible to add an intex to the data \n",
    "ind=list(range(0,28746))\n",
    "final_train=final_train.assign(Index = ind)\n",
    "#final_train\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a very simple XGboost model\n",
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 26\n",
    "param['min_child_weight']=1\n",
    "param['gamma']=0\n",
    "param['subsample']=1\n",
    "param['scale_pos_weight']=1\n",
    "param['colsample_bytree']=1\n",
    "param['learning_rate'] =0.1\n",
    "param['n_estimators']=1000\n",
    "param['seed']=123\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 1000\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist )\n",
    "# get prediction\n",
    "pred = np.int_(bst.predict( xg_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculation\n",
    "f1 = f1_score(y_test, pred,average='macro')\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A simple Gradient boosting model\n",
    "train_gbdt=final_train.replace([np.inf],[np.nan])\n",
    "train_gbdt.fillna(0,inplace=True)\n",
    "y_gbdt=pd.DataFrame(train_gbdt['class'])\n",
    "X_gbdt=train_gbdt.drop('class', axis=1)\n",
    "\n",
    "ss = ShuffleSplit(n_splits=5,random_state=123345,test_size=0.2)\n",
    "for train_index, test_index in ss.split(X_gbdt,np.ravel(y_gbdt)): \n",
    "    X_train , X_test = X_gbdt.loc[train_index,:] , X_gbdt.loc[test_index,:]\n",
    "    y_train , y_test = y_gbdt.loc[train_index] , y_gbdt.loc[test_index]\n",
    "\n",
    "gbdt = GradientBoostingClassifier(max_depth=5,subsample=0.8,n_estimators=30)\n",
    "gbdt.fit(X_train,y_train)\n",
    "pred = gbdt.predict(X_test)\n",
    "print (classification_report(pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_results = model.predict_proba(test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv(os.path.join(path,r'Rafael_submission1.csv'),header=True,index=True, index_label='id')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
